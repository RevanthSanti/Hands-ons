{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRKlO_h7gvZp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "import re\n",
        "import string\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize as nltk_sent_tokenize\n",
        "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "RuOolsLDnU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_save_wiki(title):\n",
        "  response = requests.get(\n",
        "      \"https://en.wikipedia.org/w/api.php\",\n",
        "      params={\n",
        "          \"action\":\"query\",\n",
        "          \"format\":\"json\",\n",
        "          \"titles\": title,\n",
        "          \"prop\":\"extracts\",\n",
        "          \"explaintext\": True\n",
        "      },\n",
        "  ).json()\n",
        "\n",
        "  page = next(iter(response[\"query\"][\"pages\"].values()))\n",
        "  wiki_text = page[\"extract\"]\n",
        "\n",
        "  return wiki_text"
      ],
      "metadata": {
        "id": "XcKv8QK1oxtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "VVWK-1H0puX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub(r'[^A-Za-z0-9\\s.\\(\\)[\\]{\\}]+' ,'', text)\n",
        "  text = text.lower()\n",
        "  text = \" \".join(text.split())\n",
        "  return text\n",
        "\n",
        "def count_tokens(text):\n",
        "  tokens = tokenizer.encode(text , add_special_tokens = True)\n",
        "  return len(tokens)"
      ],
      "metadata": {
        "id": "KYq-tEoCqhKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soccar_player =[\n",
        "    \"Lionel Messi\",\n",
        "    \"Cristiano Ronaldo\",\n",
        "    \"Ronaldo\",\n",
        "    \"Zinedine Zidane\",\n",
        "    \"Ronaldinho\",\n",
        "    \"Robert Lewandowski\",\n",
        "    \"Lothar Matthäus\",\n",
        "    \"Marco van Basten\",\n",
        "    \"Roberto Baggio\",\n",
        "    \"Romário\",\n",
        "    \"George Weah\",\n",
        "    \"Rivaldo\",\n",
        "    \"Luís Figo\",\n",
        "    \"Fabio Cannavaro\",\n",
        "    \"Kaká\",\n",
        "    \"Luka Modrić\",\n",
        "]\n",
        "\n",
        "data = []\n",
        "\n",
        "for player in soccar_player:\n",
        "  info = fetch_and_save_wiki(player)\n",
        "  tokens = tokenizer.encode(info , add_special_tokens = True , truncation= True , max_length = 30000)\n",
        "  num_tokens = len(tokens)\n",
        "  data.append([player , info , num_tokens])\n"
      ],
      "metadata": {
        "id": "mNRUDJjJrfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "bamcbuIRuNW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data , columns = [\"soccar_player\" , \"player_information\" , \"num_tokens\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VgaZuTWHs_-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"world_soccar_player.csv\")"
      ],
      "metadata": {
        "id": "4q_z60D9ubCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['player_information'][0]"
      ],
      "metadata": {
        "id": "9q1EsIdzuhT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"player_clean_information\"] = df['player_information'].apply(clean_text)"
      ],
      "metadata": {
        "id": "g8_wHNnDwTGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Qi6RmL0PxlgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['player_clean_information'] = df['player_clean_information'].astype(str)"
      ],
      "metadata": {
        "id": "SpccBPaexnCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenization:\n",
        "def sent_tokenize(text):\n",
        "  sents = nltk_sent_tokenize(text)\n",
        "  # print(sents)\n",
        "  sent_filtered = []\n",
        "  for s in sents:\n",
        "    sent_filtered.append(s)\n",
        "  return sent_filtered\n",
        "\n",
        "#Cleaned sentance function\n",
        "def cleanup_sentences(text):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  sentences = sent_tokenize(text)\n",
        "  sentences_cleaned = []\n",
        "  for sent in sentences:\n",
        "    words = nltk_word_tokenize(sent)\n",
        "    words = [w for w in words if w not in string.punctuation]\n",
        "    words = [w for w in words if not w.lower() in stop_words]\n",
        "    words = [w.lower() for w in words]\n",
        "    sentences_cleaned.append(\" \".join(words))\n",
        "  return sentences_cleaned"
      ],
      "metadata": {
        "id": "y2VmqD-Ixrxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_sentences'] = df[\"player_clean_information\"].apply(cleanup_sentences)"
      ],
      "metadata": {
        "id": "uc7RcdUcyy7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uz-lxTsxy9d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf\n",
        "\n",
        "def get_tf_idf(sentences):\n",
        "  vectorizer = CountVectorizer()\n",
        "  sent_word_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "  transformer = TfidfTransformer(norm = None , sublinear_tf= False , smooth_idf= False)\n",
        "  tfidf = transformer.fit_transform(sent_word_matrix)\n",
        "  tfidf = tfidf.toarray()\n",
        "\n",
        "  # Calculate the centroid vector\n",
        "  centroid_vector = tfidf.sum(axis = 0)\n",
        "  centroid_vector = np.divide(centroid_vector , centroid_vector.max())\n",
        "\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "  relevant_vector_indices = np.where(centroid_vector > 0.3)[0]\n",
        "\n",
        "  word_list = [feature_names[idx] for idx in relevant_vector_indices]\n",
        "\n",
        "  return word_list"
      ],
      "metadata": {
        "id": "LT3d4rTH1C4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['most_important_words'] = df['cleaned_sentences'].apply(get_tf_idf)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "y5Jq8ezC1Nf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_vector_cache(sentences , embedding_model):\n",
        "  word_vectors = dict()\n",
        "  for sent in sentences:\n",
        "    words = nltk_word_tokenize(sent)\n",
        "    for w in words:\n",
        "      word_vectors.update({w: embedding_model.wv[w]})\n",
        "    return word_vectors"
      ],
      "metadata": {
        "id": "Nxey8rDV749d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_representation(words , word_vectors ,embedding_model):\n",
        "  embedding_representation = np.zeros(embedding_model.vector_size , dtype = \"float32\")\n",
        "  word_vectors_key = set(word_vectors.keys())\n",
        "\n",
        "  count = 0\n",
        "  for w in words:\n",
        "    if w in word_vectors_key:\n",
        "      embedding_representation = embedding_representation + word_vectors[w]\n",
        "      count += 1\n",
        "  if count != 0:\n",
        "    embedding_representation = np.divide(embedding_representation , count)\n",
        "  return embedding_representation\n"
      ],
      "metadata": {
        "id": "MkajZ_v77-5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(v1 , v2):\n",
        "  score = 0.0\n",
        "  if np.count_nonzero(v1) != 0 and np.count_nonzero(v2) != 0:\n",
        "    score = ((1 - cosine(v1,v2)) + 1) / 2\n",
        "  return score"
      ],
      "metadata": {
        "id": "41U6CvXdA3mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text , embedding_model):\n",
        "  raw_sentences = sent_tokenize(text)\n",
        "  clean_sentences = cleanup_sentences(text)\n",
        "  for i , s in enumerate(raw_sentences):\n",
        "    print(i,s)\n",
        "  for i , s in enumerate(clean_sentences):\n",
        "    print(i,s)\n",
        "\n",
        "  centroid_words = get_tf_idf(clean_sentences)\n",
        "  print(len(centroid_words) , centroid_words)\n",
        "  word_vectors = word_vector_cache(clean_sentences , embedding_model)\n",
        "  #centroid embedding representation\n",
        "  centroid_vector = build_embedding_representation(centroid_words , word_vectors ,embedding_model)\n",
        "  sentences_score = []\n",
        "\n",
        "  for i in range(len(clean_sentences)):\n",
        "    scores = []\n",
        "    words = clean_sentences[i].split()\n",
        "\n",
        "    #Sentence embedding representation\n",
        "    sentence_vector = build_embedding_representation(words , word_vectors ,embedding_model)\n",
        "\n",
        "    #Cosine Similarity between sentence vector and centroid vectors\n",
        "    score = similarity(sentence_vector ,centroid_vector )\n",
        "    sentences_score.append((i ,raw_sentences[i] , score , sentence_vector))\n",
        "\n",
        "  sentences_score_sort = sorted(sentences_score , key = lambda el: el[2] , reverse = True)\n",
        "\n",
        "  for s in sentences_score_sort:\n",
        "    print(s[0] , s[1] , s[2])\n",
        "\n",
        "  count = 0\n",
        "  sentence_summary = []\n",
        "  for s in sentences_score_sort:\n",
        "    if count > 100:\n",
        "      break\n",
        "    include_flag = True\n",
        "    for ps in sentence_summary:\n",
        "      sim = similarity(s[3] , ps[3])\n",
        "      if sim > 0.95:\n",
        "        include_flag = False\n",
        "    if include_flag:\n",
        "      sentence_summary.append(s)\n",
        "      count += len(s[1].split())\n",
        "\n",
        "    sentence_summary = sorted(sentence_summary , key = lambda el: el[0] , reverse = False)\n",
        "\n",
        "  summary = \"\\n\".join(s[1] for s in sentence_summary)\n",
        "  # print(summary)\n",
        "  return summary"
      ],
      "metadata": {
        "id": "l4biDkRo-ZfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_sentences'] = df['cleaned_sentences'].astype(str)\n",
        "\n",
        "sentences = [nltk.word_tokenize(sent) for sent in df['cleaned_sentences'].values]\n",
        "\n",
        "model = Word2Vec(sentences , min_count = 1 , sg = 1)\n",
        "\n",
        "df['summary'] = df['cleaned_sentences'].apply(lambda x:summarize(x ,model))"
      ],
      "metadata": {
        "id": "k_PXfQ2aC9rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['summary'][0]"
      ],
      "metadata": {
        "id": "EIqbfn1aDG-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0RVHBwRjEFaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Summary_token_count\"] = df['summary'].apply(count_tokens)"
      ],
      "metadata": {
        "id": "UzAvS8TWgM5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import plotly.express as px\n",
        "# import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame containing 'num_tokens' and 'Summary_token_count' columns\n",
        "\n",
        "# Create the scatter plot using Plotly Express\n",
        "fig = px.scatter(df, x='num_tokens', y='Summary_token_count', opacity=0.8, size_max=32)\n",
        "\n",
        "# Update the layout to remove top and right spines\n",
        "fig.update_layout(xaxis=dict(showline=True, linecolor='black', showgrid=True, gridcolor='lightgrey'),\n",
        "                  yaxis=dict(showline=True, linecolor='black', showgrid=True, gridcolor='lightgrey'),\n",
        "                  showlegend=False)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "chO9c3CKYejc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RaUkx_R6F8Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "GdXtHOBVGNo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['summary'][3]"
      ],
      "metadata": {
        "id": "-BaW1iF8GOvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "v-UXPZX5aoFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "import sys\n",
        "sys.setrecursionlimit(10000)  # Set the recursion limit to a higher value\n",
        "\n",
        "# from rouge import Rouge\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Define lists to store ROUGE scores\n",
        "rouge_1_scores = []\n",
        "rouge_2_scores = []\n",
        "rouge_l_scores = []\n",
        "\n",
        "# Iterate through each row in the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # Get the summary and player information for the current row\n",
        "    summary = row['summary']\n",
        "    player_information = row['player_information']\n",
        "\n",
        "    # Calculate ROUGE score for the current data point\n",
        "    scores = rouge.get_scores(summary, player_information)\n",
        "\n",
        "    # Extract ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
        "    rouge_1_score = scores[0]['rouge-1']['f']\n",
        "    rouge_2_score = scores[0]['rouge-2']['f']\n",
        "    rouge_l_score = scores[0]['rouge-l']['f']\n",
        "\n",
        "    # Append scores to respective lists\n",
        "    rouge_1_scores.append(rouge_1_score)\n",
        "    rouge_2_scores.append(rouge_2_score)\n",
        "    rouge_l_scores.append(rouge_l_score)\n",
        "\n",
        "# Add new columns for ROUGE scores to the dataframe\n",
        "df['rouge_1_score'] = rouge_1_scores\n",
        "df['rouge_2_score'] = rouge_2_scores\n",
        "df['rouge_l_score'] = rouge_l_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "dsPkJnRTGZhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "rVINSj0UZYkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create a scatter plot with Plotly\n",
        "fig = px.scatter(df, x='num_tokens', y='rouge_1_score', size_max=32, opacity=0.8,\n",
        "                 title='Scatter Plot of num_tokens vs rouge_1_score',\n",
        "                 labels={'num_tokens': 'Number of Tokens', 'rouge_1_score': 'ROUGE-1 Score'})\n",
        "\n",
        "# Hide the top and right spines\n",
        "fig.update_layout(xaxis=dict(showline=True, showgrid=False, zeroline=False),\n",
        "                  yaxis=dict(showline=True, showgrid=False, zeroline=False))\n",
        "\n",
        "# Update the figure with the new column\n",
        "fig.add_scatter(x=df['num_tokens'], y=df['Summary_token_count'], mode='markers', marker=dict(color='red'), name='Summary_token_count')\n",
        "\n",
        "# Show the interactive plot\n",
        "fig.show()\n"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "BjTdoQpghWxc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg5uFMhmjHeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}